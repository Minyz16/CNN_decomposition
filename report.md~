# 毕设进展情况
## 2016/2/29
* 跑了caffe官网的一个例子 http://nbviewer.jupyter.org/github/BVLC/caffe/blob/master/examples/00-classification.ipynb
* 学习了ipython notebook基本用法（跑官网例子用）
* 尝试找kernal分解的code

## 2016/3/1
* 未找到基分解、迭代剪枝权重的代码实现
* 找到了kernal张量化的code: https://github.com/vadim-v-lebedev/cp-decomposition  官网：http://sites.skoltech.ru/compvision/projects/speeding-up-cnns/

## 2016/3/2
* 读了论文：SPEEDING-UP CONVOLUTIONAL NEURAL NETWORKS USING FINE-TUNED CP-DECOMPOSITION
* 读了该论文code（未全部完成），了解了code基本结构
* 配置该论文code所需环境

## 2016/3/3
* 了解了protobuf的使用方法，读了caffe代码中的caffe.proto,了解了用caffe写网络结构的基本流程
* 跑了官网的例子http://caffe.berkeleyvision.org/gathered/examples/mnist.html

## 2016/3/8
* 读完论文code，理解了code执行流程
* 尝试在服务器上跑通code，正在配置相关环境

## 2016/3/9
* 在实验室服务器上调通了code
* 粗略测量了一下mnist上加速后与加速前的数据，结果存放在 tensorizing_kernel/result.txt
* 学习了finetuning的例子http://caffe.berkeleyvision.org/gathered/examples/finetune_flickr_style.html

## 2016/3/10
* 跑了官网例子：CIFAR-10 tutorial
* 读完了http://caffe.berkeleyvision.org/tutorial/, 熟悉caffe，笔记写在 笔记/caffe tutorial笔记.txt
* 学习如何在caffe中添加自己的层

```
下周计划：
* 尝试找下一篇论文的代码，并调通
* 在mnist、和人脸数据集上分别测试kernel张量化方法，给出时间空间优化程度以及准确率drop
```
## 2016/3/14
* 准备详细测试cp-decomposition方法在mnist数据集上的表现
  1. 按照论文，要对加速后的model进行fine-tune （问题：调参方法未掌握，调后的网络误差更大）
  2. 想观察加速前后的网络的空间占用，尚未找到合适方法
  3. 已经测量R=4时CPU、GPU下分别加速网络的error、accuracy、time，放在tensorizing_kernel/result.txt中（随实验继续更新，为什么GPU下反而更慢？）

## 2016/3/15
* 为了修改代码，仔细读了一遍源代码,对应论文查看又发现了一些问题
* Questions
	ConvolutionParameter Pad,group？（pad是指对feature map进行padding；group还是不知道）
	每一层都需要激活函数? （caffe的convolution layer默认没有激活函数）
	bias？   （进行4层卷积之后再加上bias）
	matrix的2范数？

## 2016/3/17
* 尝试跑一下人脸数据集
* 调通该数据集并测出准确率与时间开销
* 尝试将CP分解应用在该数据集上

## 2016/3/18
* 尝试将CP分解应用在该数据集上，源代码好像有问题，尚未调通

## 2016/3/19
* 将代码成功用在了人脸数据集上
* 发现优化结果不稳定，有时快有时比原来方法还慢，查找原因

## 2016/3/20
* 测量了一些人脸数据集上的数据，数据存在tensorizing_kernel/CP-decomposition.xlsx

```
本周总结
   本周主要是对应论文详细读了一遍源码，然后将源码在新的数据集上调通。调通的过程中遇到一些问题，主要是原来的代码与mnist数据集存在一些耦合，加上我对caffe prototxt中各种参数还不是很理解，耽误了时间。调通代码之后，开始测量在人脸数据集上的表现，发现有的时候还没有原始网络快，于是又发现了一些问题，列举如下：
   1. 同一参数在不同时间测量两遍差异巨大，可能与服务器状态不一样有关
   2. tensorizing_kernel/CP-decomposition.xlsx的sheet2中，绘制了forward-time随着batch-size的变化图像。发现，time随batch并不是线性增长（意味着每幅图像的时间越来越小），有的时候还会有突变，这样导致forward-time的测量标准很模糊（因为平均时间=time/batchsize会受batchsize影响，还不知道什么时候就突变了）
   3. 经推算，只有当分解的秩R<S*T*d*d/(S+d+d+T)时，理论上卷积层才会有加速效果。开始时我分解的是conv1层，由于conv1的输入维度=1（S=1），所以只有R小于20的时候才会有效果，导致有的时候加速后的比加速前还慢。后来将分解的层改为conv2（S=96），R的范围就加大了很多。
   4. 
下周计划
* 继续测量CP分解方法在人脸数据集上的参数
* 查找其他方法
```
## 2016/3/22
* 继续测量数据







