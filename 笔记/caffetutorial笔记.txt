1.	Blob：N维向量，是data基本单位，提供CPU和GPU的同步。
const Dtype* cpu_data() const;
Dtype* mutable_cpu_data();
const Dtype* gpu_data() const;
Dtype* mutable_gpu_data();
Const方法不会改变值，所以最新数据的位置保持不变。Mutable方法可能改变值，并且将最新数据的label转到相应设备（CPU或GPU）

2.	Layer
从bottom到top
Each layer type defines three critical computations: setup, forward, and backward.
?	Setup: initialize the layer and its connections once at model initialization.
?	Forward: given input from bottom compute the output and send to the top.
?	Backward: given the gradient w.r.t. the top output compute the gradient w.r.t. to the input and send to the bottom. A layer with parameters computes the gradient w.r.t. to its parameters and stores it internally.

3.	Net
   a set of layers. A typical net begins with a data layer that loads from disk and ends with a loss layer.

Model initialization is handled by Net::Init().,call every layer’s SetUp()
验证网络正确性

4.	ModelFormat
The model format is defined by the protobuf schema in :
https://github.com/BVLC/caffe/blob/master/src/caffe/proto/caffe.proto 

5.	Solver
each iteration
1.	calls network forward to compute the output and loss
2.	calls network backward to compute the gradients
3.	incorporates the gradients into parameter updates according to the solver method
4.	updates the solver state according to learning rate, history, and method
5.	base_lr: 0.01     # begin training at a learning rate of 0.01 = 1e-2
6.	
7.	lr_policy: "step" # learning rate policy: drop the learning rate in "steps"
8.	                  # by a factor of gamma every stepsize iterations
9.	
10.	gamma: 0.1        # drop the learning rate by a factor of 10
11.	                  # (i.e., multiply it by a factor of gamma = 0.1)
12.	
13.	stepsize: 100000  # drop the learning rate every 100K iterations
14.	
15.	max_iter: 350000  # train for 350K iterations total
16.	
17.	momentum: 0.9

??dropout、momentum

